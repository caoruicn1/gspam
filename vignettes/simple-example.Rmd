---
title: "Intro to gspam"
author: "Thayer Fisher"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
header-includes:
  - \usepackage{bbm}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

gspam is a collection of solvers aimed at solving the following problem:

$$\min_f\sum_{i=1}^{n}\ell(y_i,\sum_{j=1}^{p}f_j(x_{i,j})+\lambda_1\sum_{j=1}^{p}P(f_j)+\lambda_2\sum_{j=1}^{p}\Vert f_j\Vert_n$$

This type of penalized nonparametric regression is ideal in situations where we have high-dimensional data witha  sparse, nonlinear signal. In this case, we can trade off between sparsity of the signal in the features, and complexity of the function of each feature in the mean model. More information about the statistical properties of this method will be availible here once the publication becomes publicly-available.

We solve these problems by proximal gradient descent. Currently, the package supports logistic and quadratic losses, and fused lasso, polynomial basis, and categorical variational penalties.

# Dataset

This package includes a sample data set. It has 101 features and 1000 observations. 100 of these features are normally distributed, and therefore continuous. One of them is categorical with 3 categories: "yes", "no", and "maybe". Binary and real-valued outcomes are generated based on this data. The true mean model is:

\begin{aligned}
f(X_i)&= \mathbb{1}(x_c = \text{no})-\mathbb{1}(x_{c}=\text{yes})+\sum_{j=1}^{5}\left(\mathbb{1}(x_{ij}>0)-\mathbb{1}(x_{ij}<0)\right).
\end{aligned}
For the binary case,
$$P(y_i=1| X_i) = \text{expit}(f(X_i)).$$
For the real-valued case,
$$E(y_i| X_i) = f(X_i)+\epsilon_i.$$
Where $\epsilon_i\stackrel{iid}{\sim} N(0,1)$.

The above case is a somewhat ideal scenario for this package. $n>p$, so the data are not really high-dimensional, and the mean model is non-linear and sparse, but still with a recoverable signal. This dataset will be used for illustrative purposes for these reasons.



# Examples

## A singlular fit

Using this package is very simple
```{r}
set.seed(1408)
library(gspam)
data(example_data)
print(head(example_data[,c("true_signal","y_binary","y_continuous","cat_data","V1")]))
```


We see here that the first 3 colums are the true signal, followed by binary outcomes and continuous outcomes generated from that data. The fourth column is a categorical covariate, and the following 100 columns are all real-values covariates. Lets start with a real-valued outcome. First we remove the true signal and the binary outcome. 

```{r}
train_rows <- c(1:990)
test_rows <- c(991:1000)
y_quad <- example_data[train_rows,3]
y_log <- example_data[train_rows,2]
true_signal <- example_data[,1]
x <- example_data[train_rows,c(-1,-2,-3)]

test_y_quad <- example_data[test_rows,104]
test_y_log <- example_data[test_rows,103]
test_x <- example_data[test_rows,c(-102,-103,-104)]
```

Now we can call gspam directly using the fused lasso prox, and visualize how the number of active features increases as the sparsity penalty decreases. 

```{r fig.width=6,fig.height=6}
results1 <- gspam(x,y_quad,'fl','quad')
plot(results1)
```


Encouragingly, we see a plateau at 6 active features, which is the true number of signal features, suggesting that it is "difficult" for the noise features to enter the model. We can verify this by the following:

```{r}
print(head(results1$fitted[[20]][,c(1:7)]))
```


This helps to verify that we are only fitting the true signal features in this lambda range.

## Cross-validation

The other major function in this package is gspam.cv . It too has a corresponding plot() function. The setup for crossvalidation is similar, with the default number of folds being 10. We will utilize the binary data for this example.

```{r fig.width=6,fig.height=6}
results2 <- gspam.cv(x,y_log,'fl','log')
plot(results2)
```


The results here are less promising, owing to the reduced information from binary data. There is increased variability in our estimate as lambda decreases, likely owing to the fact that we are mostly fitting noise in a large range of values. This example is illustrative of the fact that we will have many problems with binary data in the high-dimensional setting.

## Prediction

The final pair of functions are the predict functions. For solving a lambda path, you need to specify the position on the lambda path on which to predict. For a gspam.cv object, we use the lambda pair with the lowest mean loss.

```{r}
results3 <- gspam(x,y_log,'fl','log')
prediction_path <- predict(results3,test_x,50)
prediction_path
```

Now predicting using the best model from cross-validation:
```{r}
prediction_cv <- predict(results2,test_x)
prediction_cv
```


We can see that while the sign of the fits is mostly the same, the actual values are quite different. This suggests that we should use cross-validation in most cases for this method, rather than picking a random lambda on the solution path. We can easily change these to probability scale:

```{r}
print("Path:")
expit(prediction_path)
print("CV:")
expit(prediction_cv)
print("actual")
expit(true_signal[991:1000])
```

This concludes the summary of the 6 major functions currently supported by gspam.
